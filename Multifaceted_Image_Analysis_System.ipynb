{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!sudo apt install libtesseract-dev\n",
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUGK5iXq_Lir",
        "outputId": "dd35d3ee-8421-4915-a5cf-7d3a8e8b1fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libtesseract-dev is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "uCrGBsey21zE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J49JVK3f92i-"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pytesseract.pytesseract.tesseract_cmd = (r'/usr/bin/tesseract')"
      ],
      "metadata": {
        "id": "h6JAOUIb97Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_text(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Preprocessing the image to improve OCR accuracy (you can experiment with other preprocessing techniques)\n",
        "    processed_image = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
        "\n",
        "    # Perform OCR using pytesseract\n",
        "    extracted_text = pytesseract.image_to_string(processed_image)\n",
        "\n",
        "    print(\"Extracted text:\")\n",
        "    print(extracted_text)"
      ],
      "metadata": {
        "id": "WguI5uvI9-G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_faces(image_path):\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    image = cv2.imread(image_path)\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces\n",
        "    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    if len(faces) > 0:\n",
        "        print(\"Detected facial expressions:\")\n",
        "        for (x, y, w, h) in faces:\n",
        "            # Do further processing if needed (e.g., you can crop the face region)\n",
        "            print(\"Face detected at (x, y, w, h):\", x, y, w, h)"
      ],
      "metadata": {
        "id": "Ya-npubs-AQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_objects(image_path):\n",
        "    # Load pre-trained YOLO (You Only Look Once) object detection model\n",
        "    print(cv2.data.haarcascades)\n",
        "    net = cv2.dnn.readNet('./yolov3.weights', './yolov3.cfg')\n",
        "    with open('./yolov3.txt', 'r') as f:\n",
        "        classes = [line.strip() for line in f.readlines()]\n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width = image.shape[:2]\n",
        "\n",
        "    blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "\n",
        "    # Get the output layer names\n",
        "    out_layer_names = net.getUnconnectedOutLayersNames()\n",
        "    detections = net.forward(out_layer_names)\n",
        "\n",
        "    conf_threshold = 0.5\n",
        "    nms_threshold = 0.4\n",
        "\n",
        "    objects = []\n",
        "    for detection in detections:\n",
        "        for obj in detection:\n",
        "            scores = obj[5:]\n",
        "            class_id = scores.argmax()\n",
        "            confidence = scores[class_id]\n",
        "\n",
        "            if confidence > conf_threshold:\n",
        "                center_x = int(obj[0] * width)\n",
        "                center_y = int(obj[1] * height)\n",
        "                w = int(obj[2] * width)\n",
        "                h = int(obj[3] * height)\n",
        "                x = center_x - w // 2\n",
        "                y = center_y - h // 2\n",
        "                objects.append((classes[class_id], confidence, (x, y, w, h)))\n",
        "\n",
        "    if objects:\n",
        "        print(\"Detected objects:\")\n",
        "        for obj in objects:\n",
        "            print(f\"{obj[0]} (confidence: {obj[1]})\")"
      ],
      "metadata": {
        "id": "_xuG4lwV-EZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    image_path = \"./sample_data/IELTS.jpg\"  # Replace with the actual path to your image\n",
        "\n",
        "    detect_text(image_path)\n",
        "    detect_faces(image_path)\n",
        "    detect_objects(image_path)"
      ],
      "metadata": {
        "id": "rbs-DY4V-G97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beba832d-844a-4475-cb04-23d6a761548a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted text:\n",
            " \n",
            "\n",
            "â€”\n",
            "\n",
            "FROM\n",
            "HALLMARK\n",
            "\n",
            " \n",
            "\n",
            "En hallmarkimmigration.com \\ +91 8872038888\n",
            "\f\n",
            "Detected facial expressions:\n",
            "Face detected at (x, y, w, h): 222 644 111 111\n",
            "/usr/local/lib/python3.10/dist-packages/cv2/data/\n"
          ]
        }
      ]
    }
  ]
}